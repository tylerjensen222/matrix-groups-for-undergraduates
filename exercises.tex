%==============================================================================
%  Matrix Groups for Undergraduates - Exercises
%  Initial LaTeX Commit
%==============================================================================

\documentclass[12pt]{book}

%------------------------------------------------------------------------------
%   Packages
%------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\geometry{margin=1in}

\usepackage{xparse}
\usepackage{xcolor}

% Define the 'exercise' environment with an optional status argument
\NewDocumentEnvironment{taggedexercise}{O{}}
{%
  % Increment the 'exercise' theorem counter manually
  \refstepcounter{exercise}%
  
  % Print the heading in the document with the optional status
  \par\noindent
  \textbf{Exercise \theexercise}%
  \if\relax\detokenize{#1}\relax
    % If no optional argument provided, do nothing
  \else
    \textbf{\ [#1]}%
  \fi
  \quad

  % Also add it to the ToC at the subsection level (you can adjust this to 'section' or 'subsubsection')
  % If you'd rather keep them out of the main ToC and make a separate "List of Exercises," 
  % you can adapt this approach using a custom list environment or the 'thmtools' package.
  \addcontentsline{toc}{subsection}{%
    Exercise \theexercise
    \if\relax\detokenize{#1}\relax
    \else
      \ [#1]%
    \fi
  }
}{%
  \par
}


%------------------------------------------------------------------------------
%   Theorem-Like Environments
%------------------------------------------------------------------------------
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]

% If you prefer a separate counter per section:
% \newtheorem{exercise}{Exercise}[section]

% Custom solution environment
\newenvironment{solution}
{%
  \par\noindent\textbf{Solution.}\quad
}
{%
  \qed\par
}

%------------------------------------------------------------------------------
%   Metadata
%------------------------------------------------------------------------------
\title{Exercises from \textit{Matrix Groups for Undergraduates} \\
       by Kristopher Tapp}
\author{Tyler Jensen | tyjensen222@gmail.com}
\date{\today}

%------------------------------------------------------------------------------
%   Document
%------------------------------------------------------------------------------
\begin{document}


%------------------------------------------------------------------------------
%   Common Macros for Matrix Groups and Lie Algebras
%------------------------------------------------------------------------------
\newcommand{\tr}[1]{\text{trace}({#1})}

\newcommand{\R}{\mathbb{R}}        % Real numbers
\newcommand{\C}{\mathbb{C}}        % Complex numbers
\newcommand{\HH}{\mathbb{H}}        % Quarternions
\newcommand{\Q}{\mathbb{Q}}        % Rational numbers
\newcommand{\Z}{\mathbb{Z}}        % Integers
\newcommand{\N}{\mathbb{N}}        % Natural numbers
\newcommand{\K}{\mathbb{K}}        % Arbitrary field

% Common matrices
\newcommand{\mattwo}[4]{%
  \begin{pmatrix}
    #1 & #2 \\
    #3 & #4
  \end{pmatrix}%
}
\newcommand{\boldmattwo}[4]{%
  \begin{pmatrix}
    \mathbf{#1} & \mathbf{#2} \\
    \mathbf{#3} & \mathbf{#4}
  \end{pmatrix}%
}



%--- General Linear, Special Linear, etc. 
\newcommand{\GL}[2]{\mathrm{GL}_{#1}(#2)}   % e.g. \GL{n}{\R} -> GL_n(\R)
\newcommand{\SL}[2]{\mathrm{SL}_{#1}(#2)}   % e.g. \SL{n}{\C} -> SL_n(\C)

%--- Orthogonal, Special Orthogonal, Symplectic
\newcommand{\Ogroup}[1]{\mathrm{O}(#1)}     % O(n)
\newcommand{\SO}[1]{\mathrm{SO}(#1)}        % SO(n)
\newcommand{\Sp}[1]{\mathrm{Sp}(#1)}        % Sp(n) symplectic group

%--- Unitary, Special Unitary
\newcommand{\U}[1]{\mathrm{U}(#1)}          % U(n)
\newcommand{\SU}[1]{\mathrm{SU}(#1)}        % SU(n)

%--- Lie algebras
\newcommand{\lie}[1]{\mathfrak{#1}}         % \lie{g} -> \mathfrak{g}
\newcommand{\so}[1]{\mathfrak{so}_{#1}}     % so(n)
\newcommand{\sualg}[1]{\mathfrak{su}_{#1}}  % su(n)
\newcommand{\glalg}[1]{\mathfrak{gl}_{#1}}  % gl(n)
\newcommand{\slalg}[1]{\mathfrak{sl}_{#1}}  % sl(n)
\newcommand{\spalg}[1]{\mathfrak{sp}_{#1}}  % sp(n)

%--- Generic placeholders for a Lie algebra g and Lie group G
\newcommand{\g}{\mathfrak{g}}
\newcommand{\G}{\mathrm{G}}

\frontmatter
\maketitle
\tableofcontents

\mainmatter

%==============================================================================
\chapter{Matrices}
%==============================================================================

% Exercise 1.1
\begin{taggedexercise}[Complete]
Describe a natural 1-to-1 correspondence between elements of $\SO{3}$ and elements of

\[
T^1S^2 = \{ (p, v) \in \R^3 \times \R^3 : |p| = |v| = 1 \text{ and } p \perp q\}
\]

\end{taggedexercise}

\begin{solution}
Using the globe analogy from Question 1.2, fix a point $r$ to be the north pole, 
and a point $e$ that lies on the equator induced by the choice of $r$, and assert this as the arbitrary `identity'.

Next, given some $A \in \SO{3}$, identify an element in $T^1S^2$ via $A \mapsto (Ar, Av)$, 
as in first where $A$ maps the north pole $r$, and then how $A$ rotates the globe about the axis induced by $r$ and its antipodal point.
\end{solution}

% Exercise 1.2
\begin{taggedexercise}[Complete]
  Prove equation 1.3:
  \[
  (A \cdot B)^T = B^T \cdot A^T
  \]
\end{taggedexercise}

\begin{solution}
  First, 
  \[
    \begin{aligned}
      (A \cdot B)^T_{ij} &= (A \cdot B)_{ji} \\
                         &= \sum_{k=1}^n A_{jk}B_{ki}
   \end{aligned}
  \]
   Next, 
   \[
    \begin{aligned}
      (B^T \cdot A^T)_{ij} &= \sum_{k=1}^n(B^T)_{ik}(A^T)_{kj} \\
                         &= \sum_{k=1}^n B_{ki}A_{jk} \\
                         &\stackrel{*}{=} \sum_{k=1}^n A_{jk}B_{ki}
   \end{aligned}
  \]
\end{solution}

Note the $\stackrel{*}{=}$ step uses the commutativity of multiplication, 
hence the above proof does not work when $\K = \HH$.


% Exercise 1.3
\begin{taggedexercise}[Complete]
  Prove equation 1.4:
\[
\tr{A \cdot B} = \tr{B \cdot A}
\]
\end{taggedexercise}

\begin{solution}
  First,
  \[
    \begin{aligned}
      \tr{A \cdot B}_{ii} &= \sum_{i=1}^n \sum_{k=1}^n A_{ik}B_{ki} \\
   \end{aligned}
  \]
  Next,
  \[
    \begin{aligned}
      \tr{B \cdot A}_{ii} &= \sum_{i=1}^{n} (B \cdot A)_{ii} \\
                          &= \sum_{i=1}^n \sum_{k=1}^n B_{ik}A_{ki} \\
   \end{aligned}
  \]
  Carefully reindexing and resumming gives the result.
\end{solution}
Note that the `careful reindexing and resumming process' implies $\tr{}$ is invariant under cyclic permutation, e.x.:
\[
  \tr{A \cdot B \cdot C} = \tr{C \cdot A \cdot B}
\]

% Exercise 1.4
\begin{taggedexercise}[Complete]
  Let $A, B \in M_n{\K}$. Prove that if $A \cdot B = I$ then $B \cdot A = I$.
\end{taggedexercise}

\begin{solution}
  Note that
  \[
  A \cdot B = I \iff A = B^{-1}.
  \]
  The result follows.
\end{solution}

% Exercise 1.5
\begin{taggedexercise}[Complete]
  Suppose that the determinant of $A \in M_n(\HH)$ were defined as in Equation 1.5. Show for 
\[
  A = \boldmattwo{i}{j}{i}{j} \in M_n(\HH)
\]

that $\det(A) \neq 0$ but 
\[
R_A: H^2 \to H^2
\]
is not invertible.
\end{taggedexercise}

\begin{solution}
  Given the definition from Equation 1.5:
  \[
    \begin{aligned}
      \det (A) &= \det \boldmattwo{i}{j}{i}{j} \\
               &= \mathbf{i} \mathbf{j} - \mathbf{j} \mathbf{i} \\
               &= (1) - (-1) \\
               &= 2 \neq 0 \\
   \end{aligned}
  \]
  However,
  \[
  R_A((-\mathbf{i}, \mathbf{i})) = (-\mathbf{i}, \mathbf{i}) \cdot \boldmattwo{i}{j} 
                   {i}{j} = (-\mathbf{i}^2 + \mathbf{i}^2, -\mathbf{i}\mathbf{j} + \mathbf{i}\mathbf{j}) = (1 - 1, -\mathbf{k} + \mathbf{k}) = (0, 0)
  \]
  Hence, $R_A$ has a non-zero determiant, but is not invertible as the kernel is non-trivial.
  Similarly, clearly the columns of $A$ are linearly dependent.
\end{solution}

% Exercise 1.6
\begin{taggedexercise}[Complete]
  Find $B \in M_2(\R)$ such that $R_B : \R^2 \to \R^2$ is a counter-clockwise rotation through an angle $\theta$.
\end{taggedexercise}

\begin{solution}
  Note that we can `represent' both $1$ and $i$ in $M_2(\R)$ via
  \[
  1 \to \mattwo{1}{0}{0}{1} = I \text{ and } i \to \mattwo{0}{1}{-1}{0}
  \]
  where the latter works since
  \[
    \mattwo{0}{1}{-1}{0}^2 = \mattwo{-1}{0}{0}{-1} = -I
  \]
  capturing the fact that $i^2 = -1$. 
  Building on this, we can represent any $a+bi \in \C$ via 
  \[
    \rho : a+bi\mapsto a \cdot I + b \cdot \mattwo{0}{1}{-1}{0} = \mattwo{a}{b}{-b}{a}.
  \]
  Next, note that the function $f_\theta(z) = ze^{i\theta}$ rotates elements counter-clockwise in $\C$ by an angle $\theta$. 
  To see this, letting $z=re^{i\varphi}$,
  \[f_\theta(z) = ze^{i\theta} = re^{i\varphi}e^{i\theta} = re^{i(\varphi + \theta)}.\]
  Applying $\rho$ gives
  \[
  \begin{aligned}
    \rho_1(e^{i\theta}) &= \rho_1(\cos(\theta) + i\sin(\theta)) \\
                        &= \mattwo{\cos(\theta)}{\sin(\theta)}
                                  {-\sin(\theta)}{\cos(\theta)} = B
  \end{aligned}
  \]


\end{solution}

%TODO: prove the statement in the solution
% Exercise 1.7
\begin{taggedexercise}[WIP]
  Describe all elements $A \in \GL{n}{\R}$ with the property $AB=BA$ for all $B \in \GL{n}{\R}$.
\end{taggedexercise}

\begin{solution}
  Matrices $A$ that commute with all matrices in $\GL{n}{\R}$ are scalar multiples of the identity
  \[
  A = \lambda I
  \]
  where $\lambda \in \R$ and $\lambda \neq 0$.
\end{solution}

% Exercise 1.8
\begin{taggedexercise}[Complete]
  Let $\SL{2}{\Z}$ denote 2 by 2 matrices with integer entries and determinant $1$.
  Prove that $\SL{2}{\Z}$ is a subgroup of $\GL{n}{\Z}$. 
  Is $SL_n(\Z)$ a subgroup of $\GL{n}{\R}$ in general?
\end{taggedexercise}

\begin{solution}
  Fixing $A, B \in \SL{2}{\Z}$, its clear that $A \cdot B$ must have all integer entries.
  Since
  \[
  \det(AB) = \det(A) \det(B) = 1 \cdot 1 = 1
  \]
  we have that $A \cdot B \in \SL{2}{\Z}$ (closure). 
  Next, fix
  \[
    A = \mattwo{a}{b}{c}{d} \in \SL{2}{\Z}
  \]
  Using Cramer's Rule to compute the inverse of $A$ we get 
  \[
  A^{-1} = \frac{1}{ad-bc}\mattwo{d}{-b}{-c}{a} = \mattwo{d}{-b}{-c}{a} 
  \]
  where $ad-bc = 1$ since $\det A = 1$, so $A^{-1}$ has all integer entries, and is a member of $\SL{2}{\Z}$.
  Therefore, $\SL{2}{\Z}$ is a subgroup of $\GL{2}{\R}$.
\end{solution}

The critical step in the above proof is discerning that the factor extracted from $A^{-1}$ is $1/\det A = 1$, which ensures the entries of the inverse are all in $\Z$.
This factor is the same for any $n$, so $\SL{n}{\Z}$ is always a subgroup of $\GL{n}{\R}$ for all $n$.


% Exercise 1.9
\begin{taggedexercise}[Complete]
  Describe the block matrix blah blabh blabhj TODO write this out
\end{taggedexercise}
\begin{solution}
  Suppose $A$ and $B$ are block matrices in $M_n(\K)$, given by
  \[
A = \begin{pmatrix}
A_1 & 0   & \cdots & 0 \\
0   & A_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0   & 0   & \cdots & A_n
\end{pmatrix},
B = \begin{pmatrix}
  B_1 & 0   & \cdots & 0 \\
  0   & B_2 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0   & 0   & \cdots & B_n
  \end{pmatrix}
\]
where $\sum \text{dim}A_i = \text{dim}A = \sum \text{dim}B_i = \text{dim}B$, and $\text{dim} A_i = \text{dim}B_i$ for each i.
Then,
\[
  A \cdot B = \begin{pmatrix}
    A_1\cdot B_1 & 0   & \cdots & 0 \\
    0   & A_2 \cdot B_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0   & 0   & \cdots & A_n \cdot B_n
    \end{pmatrix}
\]
Which can be applied to the above question to derive a simple answer.

\end{solution}

% Exercise 1.10
\begin{taggedexercise}[Complete]
  If $G_1 \subset \GL{n_1}{\K}$ and $G_2 \subset \GL{n_2}{\K}$ are subgroups, describe a subgroup of $\GL{n_1 + n_2}{\K}$ isomorphic to $G_1 \times G_2$.
\end{taggedexercise}
\begin{solution}
  Define a map 
  \[
  \varphi : G_1 \times G_2 \to \GL{n_1 + n_2}{\K} \\
  \]
  given by
  \[
  (A_1, A_2) \mapsto \mattwo{A_1}{0}
                            {0}  {A_2}
  \]
  The image of $\varphi$ is a subset of $\GL{n_1 + n_2}{\K}$ since

  \[
  \det \mattwo{A_1}{0}
  {0}  {A_2} = \det A_1 \cdot \det A_2 \neq 0
  \]
  so $\varphi(A_1, A_2) \in \GL{n_1 + n_2}{\K}$.
  To prove $\varphi$ is a group homomorphism, observe
  \[
  \begin{aligned}
    \varphi((A_1, A_2) \cdot (B_1, B_2)) &= \varphi(A_1 \cdot B_1, A_2 \cdot B_2) \\
                                         &= \mattwo{A_1 \cdot B_1}{0}{0}{A_2 \cdot B_2} \\
                                         &= \mattwo{A_1}{0}{0}{A_2} \cdot \mattwo{B_1}{0}{0}{B_2} \\
                                         &= \varphi(A_1, A_2) \cdot \varphi(B_1, B_2)
  \end{aligned}
  \]    
  hence the result.

\end{solution}

% TODO
% Exercise 1.11
\begin{taggedexercise}[TODO]
  
\end{taggedexercise}

% Exercise 1.12
\begin{taggedexercise}[Complete]
  Show that for purely imaginary $q_1, q_2 \in \HH$, $-\Re (q_1 \cdot q_2)$ is the vector dot product in $\R^3 = \text{span}(\mathbf{i},
  \mathbf{j},\mathbf{k})$ and $\Im(q_1 \cdot q_2)$ is the vector cross-product.
\end{taggedexercise}

\begin{solution}
  First,
  \[
  \begin{aligned}
    -\Re (q_1 \cdot q_2) &= -\Re ((b_1\mathbf{i} + c_1\mathbf{j} + d_1\mathbf{k}) \cdot (b_2\mathbf{i} + c_2\mathbf{j} + d_2\mathbf{k})) \\
                         &= -\Re((- b_1b_2 - c_1c_2 - d_1d_2) + \dots) \\
                         &= b_1b_2 + c_1c_2 + d_1d_2
  \end{aligned}
  \]
  Next, 
  \[
  \begin{aligned}
    \Im (q_1 \cdot q_2) &= \Im ((b_1\mathbf{i} + c_1\mathbf{j} + d_1\mathbf{k}) \cdot (b_2\mathbf{i} + c_2\mathbf{j} + d_2\mathbf{k})) \\
                        &= (c_1d_2 - d_1c_2)\mathbf{i} + (d_1b_2 - b_1d_2)\mathbf{j} + (b_1c_2 - c_1b_2)\mathbf{j}
  \end{aligned}
  \]
  Mapping $\text{span}(\mathbf{i},\mathbf{j},\mathbf{k})$ to the standard basis in $\R^3$ gives both desired results.
\end{solution}

% TODO
% Exercise 1.13
\begin{taggedexercise}[WIP]
  Prove that non-real elements $q_1, q_2 \in \HH$ commute if and only if their imaginary parts are parallel; that is, $\Im(q_1) = \lambda \cdot \Im(q_2)$ for some $\lambda \in \R$.
\end{taggedexercise}

\begin{solution}
  $(\implies)$ Let $\Im(q_1) = \lambda \cdot \Im(q_2)$ for some $\lambda \in \R$, so that
  \[
    \Im(q_1) = b\mathbf{i} + c\mathbf{j} + d\mathbf{k} = \lambda b\mathbf{i} + \lambda c\mathbf{j} + \lambda d\mathbf{k} = \Im(q_2)
  \]
  then,
  \[
  \begin{aligned}
    q_1 \cdot q_2 &= (a_1 + b\mathbf{i} + c\mathbf{j} + d\mathbf{k})\cdot (a_2 + \lambda b\mathbf{i} + \lambda c\mathbf{j} + \lambda d \mathbf{k}) 
    \\
    &=
    \bigl(a_1 a_2 - \lambda\,(b^2 + c^2 + d^2)\bigr)
    b(a_1 \lambda + a_2)\mathbf{i}
    c(a_1 \lambda + a_2)\mathbf{j}
    d(a_1 \lambda + a_2)\mathbf{k} \\
    &= 
    (a_2 + \lambda\,b\,\mathbf{i} + \lambda\,c\,\mathbf{j} + \lambda\,d\,\mathbf{k}) 
    \cdot (a_1 + b\,\mathbf{i} + c\,\mathbf{j} + d\,\mathbf{k}) \\
    &= q_2 \cdot q_1.
    \end{aligned}
  \]
  $(\impliedby)$ Let $q_1 \cdot q_2 = q_2 \cdot q_1$ where
  \[
    q_1 = (a_1 + b_1\mathbf{i} + c_1\mathbf{j} + d_1\mathbf{k}), q_2 = (a_2 + b_2\mathbf{i} + c_2\mathbf{j} + d_2\mathbf{k}).
  \]
  Then the following equalities must hold:

\end{solution}

% TODO
% Exercise 1.14
\begin{taggedexercise}[TODO]
  Charachterize the pairs $q_1, q_2 \in \HH$ which anti-commute, that is $q_1q_2 = -q_2q_1$.
\end{taggedexercise}

% Exercise 1.15
\begin{taggedexercise}[Complete]
  If $q \in \HH$ satisfies $q\mathbf{i} = \mathbf{i}q$, prove that $q \in \C$.
\end{taggedexercise}
\begin{solution}
  Let $q = a + b\mathbf{i} + c\mathbf{j} + d\mathbf{k}$. Then,
  \[
    q\mathbf{i} = a\mathbf{i} + b\mathbf{i}\mathbf{i} + c\mathbf{j}\mathbf{i} + d\mathbf{k}\mathbf{i} = -b + a\mathbf{i} + d\mathbf{j} - c\mathbf{k}
  \]
  and
  \[
    \mathbf{i}q = \mathbf{i}a + b\mathbf{i}\mathbf{i} + c\mathbf{i}\mathbf{j} + d\mathbf{i}\mathbf{k} = -b + a\mathbf{i} - d\mathbf{j} + c\mathbf{k}.
  \]
  Identifying terms gives
  \[
  d = -d \implies d = 0 
  \]
  \[
  c = -c \implies c = 0
  \]
  hence $q = a+bi \in \C$.

\end{solution}

% Exercise 1.16
\begin{taggedexercise}[Complete]
  Prove that complex multiplication in $\C \approxeq \R^2$ does not extend to a multiplication operation on $\R^3$ that makes $\R^3$ into a real division algebra.
\end{taggedexercise}

\begin{solution}
  Assume such an extension exists. 
  Consider the map analagous to the extension of $\R^2$ given by
  \[
  (a,b,c) \mapsto a + b\mathbf{i} + c \mathbf{j}
  \]
  with $\mathbf{i}^2 = \mathbf{j}^2 = -1$. 
  Then there must exist a linear map 
  \[
  T: \R^3 \to \R^3
  \]
  with $T^2 = -I$. 
  Represent $T$ by a $3 \times 3$ real matrix $M$. 
  Then $-1$ is in the spectrum of $M^2$ since $M^2 = -I$, thus $\pm i$ is in the spectrum of $M$.
  Since $\det(M) = \prod \lambda_k$ where $\lambda_k$ is an eigenvalue of $M$, there must exist some real value $\lambda$ such that
  \[
  \det(M) = (i)(-i)(\lambda) = \lambda
  \]
  where $\lambda$ must be in $\R$ since complex eigenvalues come in pairs.
  Thus, we must have
  \[
  \det(M^2) = \det(-I) = -1
  \]
  and
  \[
  \det(M^2) = \det(M)^2 = \lambda^2
  \]
  thus $\lambda^2 = -1$, which contradicts $\lambda$ being real.
  
\end{solution}

% Exercise 1.17
\begin{taggedexercise}[Complete]
  Describe a subgroup of $\GL{n+1}{\R}$ that is isomorphic to $\R^n$ under vector-addition.
\end{taggedexercise}

\begin{solution}
  Consider the matrices of the form 
  \[
  \begin{pmatrix}
  1      & x_1    & x_2    & \cdots & x_n \\
  0      & 1      & 0      & \cdots & 0   \\
  0      & 0      & 1      & \cdots & 0   \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0      & 0      & 0      & \cdots & 1
  \end{pmatrix} \in \GL{n+1}{\R}
  \]
  and the map that takes such matrices to $(x_1, \dots , x_n) \in \R^n$.

\end{solution}

% Exercise 1.18
\begin{taggedexercise}[Complete]
  If $\lambda \in \HH$ commutes with every element of $\HH$, prove that $\lambda \in \R$.
\end{taggedexercise}

\begin{solution}
  Let $\lambda$ have the property that $\lambda \cdot w = w \cdot \lambda$ for all $w \in \HH$.
  Letting $w = \mathbf{i}$, $\lambda \in \C$ per Exercise 1.15.
  Letting $\lambda = a+b\mathbf{i}$ and $w = \mathbf{j}$, we must have
  \[
    (a+b\mathbf{i}) \cdot \mathbf{j} = a\mathbf{j} + b\mathbf{k} = a\mathbf{j} - b\mathbf{k} = \mathbf{j}(a+b \mathbf{i})
  \]
  hence $b = -b \implies b = 0$, therefore $\lambda = a \in \R$.
  
\end{solution}

%==============================================================================
\chapter{All matrix groups are real matrix groups}
%==============================================================================
\section{Exercises}


%==============================================================================
\chapter{The orthogonal groups}
%==============================================================================
\section{Exercises}

% Exercise 3.1
\begin{taggedexercise}[Complete]
  Prove part (4) of Proposition 3.3:
  \[
  \overline{\langle X, Y \rangle} = \langle Y, X \rangle
  \]
\end{taggedexercise}
\begin{solution}
  \[
  \begin{aligned}
    \overline{\langle X, Y \rangle} &= \overline{\langle (x_1, \dots , x_n), (y_1, \dots , y_n) \rangle} \\
                                    &= \overline{x_1\bar{y_1} + \dots + x_n\bar{y_n}} \\
                                    &= \overline{x_1 \bar{y_1}} + \dots + \overline{x_n \bar{y_n}} \\
                                    &= y_1\bar{x_1} + \dots + y_n\bar{x_n} \\
                                    &= \langle (y_1, \dots , y_n), (x_1, \dots , x_n) \rangle \\
                                    &= \langle Y, X \rangle
  \end{aligned}
  \]
\end{solution}


% Exercise 3.2
\begin{taggedexercise}[Complete]
  Prove Equations 3.5 and 3.6:
  \[
    (3.5) :  \langle X, Y \rangle_\C = \langle f(X), f(Y) \rangle_\R + \mathbf{i} \langle f(X), f(\mathbf{i}Y) \rangle_\R
  \]
  \[
    (3.6) : |X|_\C = |f(X)|_R
  \]
  where
  \[
  f = f_n: \C^{n} \to \R^{2n}
  \]
  is given by
  \[
  f(a_1 + b_1\mathbf{i}, \dots , a_n+b_n\mathbf{i}) := (a_1, b_1, \dots, a_n, b_n).
  \]
\end{taggedexercise}

\begin{solution} First, for Equation 3.5,
  \[
  \begin{aligned}
      \langle X, Y \rangle_\C &= \langle (a_1 + b_1\mathbf{i}, \dots , a_n + b_n\mathbf{i}), (c_1+d_1\mathbf{i}, \dots , c_n+d_n\mathbf{i}) \rangle_\C \\
                              &= (a_1 + b_1\mathbf{i})\overline{(c_1+d_1\mathbf{i})} + \dots + (a_n + b_n\mathbf{i})\overline{(c_n+d_n\mathbf{i})} \\
                              &= (a_1 + b_1\mathbf{i})(c_1-d_1\mathbf{i}) + \dots + (a_n + b_n\mathbf{i})(c_n-d_n\mathbf{i}) \\
                              &= [(a_1c_1 + b_1d_1) + (-a_1d_1 + b_1c_1)\mathbf{i}] + \dots + [(a_nc_n + b_nd_n) + (-a_nd_n + b_nc_n)\mathbf{i}] \\
                              &= (a_1c_1 + b_1d_1 + \dots + a_nc_n + b_nd_n) + (-a_1d_1 + b_1c_1 + \dots - a_nd_n + b_nc_n)\mathbf{i} \\
                              &= \langle (a_1, b_1, \dots , a_n, b_n), (c_1, d_1, \dots c_n, d_n) \rangle_\R + \mathbf{i}\langle (a_1, b_1, \dots , a_n, b_n), (-d_1, c_1, \dots , -d_n, c_n) \rangle_\R \\
                              &= \langle f(X), f(Y) \rangle_\R + \mathbf{i}\langle f(X), \underbrace{f(\mathbf{i}Y)}_! \rangle_\R
    \end{aligned}
  \]
  where the equality of $\underbrace{f(\mathbf{i}Y)}_!$ is due to
  \[
  \begin{aligned}
    f(\mathbf{i}Y) &= f(\mathbf{i}(c_1 + d_1\mathbf{i}, \dots , c_n + d_n\mathbf{i})) \\
                   &= f((-d_1 + \mathbf{i}c_1, \dots, -d_n + \mathbf{i}c_n)) \\
                   &= (-d_1, c_1, \dots, -d_n, c_n)
  \end{aligned}
  \]
  Next, for Equation 3.6,
  \[
  \begin{aligned}
    |X|_\C &= \sqrt{\langle X, X \rangle_\C} \\
           &= \sqrt{\langle (a_1 + b_1\mathbf{i}, \dots , a_n + b_n\mathbf{i}), (a_1 + b_1\mathbf{i}, \dots , a_n + b_n\mathbf{i}) \rangle_\C} \\
           &= \sqrt{(a_1 + b_1\mathbf{i})(a_1 - b_1\mathbf{i}) + \dots + (a_n + b_n\mathbf{i})(a_n - b_n\mathbf{i})} \\
           &= \sqrt{a_1^2 + b_1^2 + \dots + a_n^2 + b_n^2} \\
           &= \sqrt{\langle (a_1, b_1, \dots, a_n, b_n), (a_1, b_1, \dots, a_n, b_n) \rangle}\\
           &= \sqrt{\langle f(X), f(X) \rangle}_\R \\
           &= |f(X)|_\R
  \end{aligned}
  \]
\end{solution}

% TODO
% Exercise 3.3
\begin{taggedexercise}[WIP]
  Prove Proposition 3.5, that is $\{X_1, \dots X_n\} \in \C^n$ is an orthonormal basis if and only if $\{f(X_1), f(\mathbf{i}X_1), \dots ,f(X_n), f(\mathbf{i}X_n)\}$ is an othonormal basis of $\R^{2n}$. 
\end{taggedexercise}

\begin{solution}
  ($\implies$) Let $\{Y_1, \dots Y_{2n}\} = \{f(X_1), f(\mathbf{i}X_1), \dots ,f(X_n), f(\mathbf{i}X_n)\}$ be an orthonorml basis of $\R^{2n}$.
  Then,
  \[
  \langle Y_i, Y_j \rangle_\R = \delta_{ij}
  \]
  where $\delta_{ij}$ is the Kronecker Delta.
  Next, consider
  \[
  \begin{aligned}
    \langle f^{-1}(Y_i), f^{-1}(Y_j) \rangle_\R &= f^{-1}()
  \end{aligned}
  \]
  ($\impliedby$) Let $X, Y \in \C^n$ be orthogonal.
  Then,
  \[
  \langle X, Y \rangle_\C = \langle f(X), f(Y) \rangle_\R + \mathbf{i} \langle f(X), f(\mathbf{i}Y) \rangle_\R = 0
  \]
  Since $\langle f(X), f(\mathbf{i}Y) \rangle_\R \in \R$, and hence $\mathbf{i}\langle f(X), f(\mathbf{i}Y) \rangle_\R \in \C$, both factors must vanish.
  Hence, $f$ maps $\C^n$ to an orthonormal basis of $\R^{2n}$.

\end{solution}


% Exercise 3.4
\begin{taggedexercise}[Complete]
  Prove Proposition 3.18, that is, for any $X \subset \R^n$, $\text{Symm}^+(X) \subset \text{Symm}(X)$ is a subgroup with index $1$ or $2$.
\end{taggedexercise}

\begin{solution}
  First, to show $\text{Symm}^+(X)$ is a subgroup, let $A, B \in \text{Symm}^+(X)$. 
  Then,
  \[
  \det(AB) = \det(A)\det(B) = 1 \cdot 1 = 1 \implies AB \in \text{Symm}^+(X)
  \]
  and
  \[
  1 = \det(I) = \det(AA^{-1}) = \det(A)\det(A^{-1}) = \det(A^{-1}) \implies A{-1} \in \text{Symm}^+(X)
  \]
  so $\text{Symm}^+(X)$ is a subgroup. 
  It's clear that $\text{Symm}^+(X)$ has at most $2$ cosets given the symmetry of the determinant, and has $1$ if $\text{Symm}^+(X) = \text{Symm}(X)$, hence it's index is $1$ or $2$.
\end{solution}

% Exercise 3.5
\begin{taggedexercise}[Complete]
  Let $A \in \GL{n}{\K}$. Prove that $A \in \text{O}_n(\K)$ if and only if the columns of $A$ are an orthonormal basis of $\K^n$.
\end{taggedexercise}

\begin{solution}
  ($\implies$) Let the columns of $A$ form an orthonormal basis $\{ v_1, \dots, v_n \}$ of $\K^n$.
  Thus,
  \[
  \begin{aligned}
    \langle v_i, v_j \rangle &= \delta_{ij}
  \end{aligned}
  \]
  where $\delta_{ij}$ is the Kronecker Delta.
  Next,
  \[
    (A^*A)_{ij} = \sum_{k = 1}^n A^*_{ij}A_{kj} = \sum_{k = 1}^n A_{ki}A_{kj}
  \]
  where $A_{ki}$ is the $k$th component of $v_i$, hence
  \[
    (A^*A)_{ij} = \langle v_i, v_j \rangle = \delta_{ij} \implies A^*A = I
  \] 

  ($\impliedby$) Fix some $A \in \text{O}_n(\K)$ with columns $\{ v_1, \dots, v_n \}$.
  Since $A$ is an element of a group, $A^{-1}$ is defined, and so the columns are linearly independent.
  Next, 
  \[
  \begin{aligned}
    (I)_{ij} &= (A^*A)_{ij} \\
             &= \langle v_i, v_j \rangle \\
             &= \delta_{ij}
  \end{aligned}
  \]     
  Hence, the columns are orthonormal.


\end{solution}

% Exercise 3.6
\begin{taggedexercise}[Complete]
  \begin{enumerate}
    \item Show that for every $A \in \Ogroup{2} - \SO{2}$, $R_A: \R^2 \to \R^2$ is a flip about some line through the origin. How is this line determined by the angle of $A$?
    \item Let $B = \mattwo{\cos \theta}{\sin \theta}{-\sin\theta}{\cos\theta} \in \SO{2}$. Assume $\theta$ is not an integer multiple of $\pi$. Prove that $B$ does not commute with any $A \in \Ogroup{2} - \SO{2}$.
  \end{enumerate}
\end{taggedexercise}

\begin{solution}
  (1). First, note that given some $X = (x_1, x_2) \in \R^2$,
  \[
    (x_1, x_2)\mattwo{1}{0}{0}{-1} = (x_1, -x_2)
  \]
  hence, $\mattwo{1}{0}{0}{-1}$ corresponds to a flip through the $x$-axis.
  Per Equation 3.8, we have
  \[
  A = \mattwo{\cos\theta}{\sin\theta}{\sin\theta}{-\cos\theta}
  \]
  where $\theta \in [0, 2\pi)$.
  Using this, observe that
  \[
    A = \mattwo{\cos\theta}{\sin\theta}{\sin\theta}{-\cos\theta} = \mattwo{1}{0}{0}{-1} \mattwo{\cos \theta}{\sin \theta}{-\sin\theta}{\cos\theta}
  \]
  where the second factor on the RHS is in $\SO{2}$ and corresponds to a counter-clockwise rotation of $\theta$.
  Hence, $A$ is a rotation flip then rotation.

  (2) Let $B \in \SO{2}$. Then 
  \[
  R_{AB}((x_1, x_2)) = XAB = (x_{1, \theta}, -x_{2, \theta})B = (x_{1, \theta + \varphi}, -x_{2, \theta + \varphi})
  \]
  is given by the flip of the first factor of $A$, then the rotations $\theta$ and $\varphi$ of the next two factors.
  However,
  \[
  R_{BA}((x_1, x_2)) = XBA = (x_{1, \varphi}, x_{2, \varphi})A.
  \]
  In this case, $x_{2, \varphi}$ will first be flipped and then rotated by $\theta$, and so we cannot assert that it is equal to $-x_{2, \varphi + \theta}$, hence the matrices cannot commute.

\end{solution}

% TODO
% Exercise 3.7
\begin{taggedexercise}[WIP]
  Describe the product of any two elements in $O(2)$ in terms of their angles.
\end{taggedexercise}

\begin{solution}
  Let $A, B \in \SO{2}$ and $C, D \in O(2) - \SO{2}$.
  We need to describe $R_{AB}$, $R_{CD}$, and $R_{AC}$.
  First, 
  \[
  \begin{aligned}
    R_{AB} &= \mattwo{\cos \theta}{\sin \theta}{-\sin\theta}{\cos\theta} \mattwo{\cos \varphi}{\sin \varphi}{-\sin\varphi}{\cos\varphi} \\
           &= \mattwo{\cos\theta\cos\varphi - \sin\theta\sin\varphi}{-\sin\theta\cos\varphi - \sin\varphi\cos\theta}{\cos\theta\sin\varphi + \cos\varphi\sin\theta}{-\sin\theta\sin\varphi + cos\theta\cos\varphi} \\
           &= \mattwo{\cos(\theta + \varphi)}{\sin(\theta + \varphi)}{-\sin(\theta + \varphi)}{\cos(\theta + \varphi)}
  \end{aligned}
  \]
\end{solution}



%==============================================================================
\chapter{The topology of matrix groups}
%==============================================================================
\section{Exercises}

%TODO
% Exercise 4.1
\begin{taggedexercise}[TODO]
Another exercise goes here.
\end{taggedexercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\chapter{Lie algebras}
%==============================================================================
\section{Exercises}

%TODO
% Exercise 5.1
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.2
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.3
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.4
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.5
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.6
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.7
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.8
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.9
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.10
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.11
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.12
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.13
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.14
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.15
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%TODO
% Exercise 5.16
\begin{taggedexercise}[TODO]

\end{taggedexercise}

%==============================================================================
\chapter{Matrix exponentiation}
%==============================================================================
\section{Exercises}

% TODO
% Exercise 6.1
\begin{taggedexercise}[WIP]
Prove Proposition 6.5, that is, suppose that $\sum A_l$ and $\sum B_l$ converge, at least one absolutely.
Let $C_l = \sum_{k=0}^l A_kB_{l-k}$. Prove that
\[
\sum C_l = (\sum A_l)(\sum B_l)
\]
\end{taggedexercise}

\begin{solution}
\[
\begin{aligned}
  (\sum C_l)_{ij} &= (\sum_{k=0}^l A_kB_{l-k})_{ij} \\
                  &= (\sum_{k=0}^l \sum_{r=1}(A_k)_{})
\end{aligned}
\]
\end{solution}

% Exercise 6.2
\begin{taggedexercise}[Complete]
  Prove that $(e^A)^* = e^{A^*}$ for all $A \in M_n(\K)$.
\end{taggedexercise}

\begin{solution}
  Using the fact that $(XY)^* = Y^*X^*$ and $(X + Y)^* = X^* + Y^*$,
  \[
  \begin{aligned}
    e^{A^*} &= \sum_{k = 0}^\infty \frac{(A^*)^k}{k!} \\
            &= \sum_{k = 0}^\infty \frac{\overbrace{A^* \cdot A^* \cdots A^*}^{k-times}}{k!} \\
            &= \sum_{k = 0}^\infty \frac{(\overbrace{A \cdot A \cdots A}^{k-times})^*}{k!} \\
            &= \sum_{k = 0}^\infty \frac{(A^k)^*}{k!} \\
            &= \left(\sum_{k = 0}^\infty \frac{(A^k)}{k!}\right)^* \\
            &= (e^A)^*
  \end{aligned}
  \]
\end{solution}

% Exercise 6.3
\begin{taggedexercise}[Complete]
  \begin{enumerate}
    \item Let $A = \text{diag}(a_1, \dots, a_n) \in M_n(\R)$. Calculate $e^A$ and give a simple prove that $\det(e^A) = e^{\text{trace}(A)}$ when $A$ is diagonal.
    \item Give a simple proof that $\det(e^A) = e^{\text{trace}(A)}$ when $A$ is conjugate to a diagonal matrix.
  \end{enumerate}
\end{taggedexercise}

\begin{solution}
  (1). Let $A = \text{diag}(a_1, \dots, a_n) \in M_n(\R)$.
  Then,
  \[
  \begin{aligned}
    e^{A} &= \sum_{k=0}^\infty \frac{\text{diag}(a_1, \dots, a_n)^k}{k!} \\
          &= \sum_{k=0}^\infty \frac{\text{diag}(a_1^k, \dots, a_n^k)}{k!} \\
          &= \text{diag}\left(\sum_{k=0}^\infty \frac{a_1^k}{k!}, \dots , \sum_{k=0}^\infty \frac{a_n^k}{k!} \right) \\
          &= \text{diag}(e^{a_1}, \dots , e^{a_n}).
  \end{aligned}
  \]
  Thus,
  \[
  \begin{aligned}
  \det(e^A) &= \det \left(\text{diag}(e^{a_1}, \dots , e^{a_n})\right)\\
            &= \prod_{i = 1}^{n}e^{a_i} \\
            &= e^{\sum_{j=1}^n a_j} \\
            &= e^{\tr{A}}
  \end{aligned}
  \]

  (2). Let $A \in M_n(\R)$ be conjugate to a diagonal matrix, that is there exists a diagonal matrix $D = \text{diag}(d_1, \dots, d_n)$ and an invertible matrix $P$ such that $A = PDP^{-1}$.
  Then,
  \[
  \begin{aligned}
    e^A &= \sum_{k=0}^\infty \frac{A^k}{k!} \\
        &= \sum_{k=0}^\infty \frac{(PDP^{-1})^k}{k!} \\
        &= \sum_{k=0}^\infty \frac{\overbrace{(PDP^{-1}PDP^{-1} \cdots PDP^{-1})}^{k-times}}{k!} \\
        &= \sum_{k=0}^\infty \frac{\overbrace{PD(P^{-1}P)D(P^{-1}P) \cdots (P^{-1}P)DP^{-1})}^{k-times}}{k!} \\
        &= \sum_{k=0}^\infty \frac{PD^kP^{-1}}{k!} \\
        &= P \left( \sum_{k=0}^\infty \frac{D^k}{k!} \right) P^{-1} \\
        &= P e^D P^{-1}.
  \end{aligned}
  \]
  Finishing it off:
  \[
  \begin{aligned}
    \det(e^A) &= \det(Pe^DP^{-1}) \\
              &= \det(P) \cdot \det(e^D) \cdot \det(P^{-1}) \\
              &= e^{\tr{D}} \\
              &= e^{\tr{A}}
  \end{aligned}
  \]
  where the last equality holds since similar matrices have the same trace. 
\end{solution}

% Exercise 6.4
\begin{taggedexercise}[WIP]
  Let $A = \mattwo{0}{1}{-1}{0}$. Compute $e^{tA}$ for arbitrary $t \in \R$.
\end{taggedexercise}

% Needs fix! See indexing of factorial terms
\begin{solution}
  First, consider the following table:

  \begin{table}[ht]
    \centering
    \begin{tabular}{c|c}

    $k$ & $A^k$  \\ 
    \hline
    0 & $I$ \\
    1 & $A$ \\
    2 & $\mattwo{-1}{0}{0}{-1}$ \\
    3 & $\mattwo{0}{-1}{1}{0}$ \\
    4 & $I$ \\
    \end{tabular}
    \end{table}

  Hence, $A$ has periodicity $4$. 
  Therefore,
  \[
  \begin{aligned}
    e^{tA} &= \sum_{k=0}^\infty \frac{(tA)^k}{k!} \\
           &= \sum_{k=0}^\infty \frac{t^kA^k}{k!} \\
           &= \sum_{i=0}^\infty \frac{t^{4i}}{i!}I + \sum_{j=0}^\infty \frac{t^{4j + 1}}{j!}A^{4j+1} + \sum_{l=0}^\infty \frac{t^{4l + 2}}{l!}A^{4l+1} + \sum_{r=0}^\infty \frac{t^{4r + 3}}{r!}A^{4r+3} \\
           &= \sum_{i=0}^\infty \frac{t^{4i}}{i!}I + \sum_{j=0}^\infty \frac{t^{4j + 1}}{j!}A + \sum_{l=0}^\infty \frac{t^{4l + 2}}{l!}A^2 + \sum_{r=0}^\infty \frac{t^{4r + 3}}{r!}A^3 \\
           &= \sum_{i=0}^\infty \frac{(t^{4})^i}{i!}I + t\sum_{j=0}^\infty \frac{(t^{4})^j}{j!}A + t^2\sum_{l=0}^\infty \frac{(t^4)^l}{l!}A^2 + t^3\sum_{r=0}^\infty \frac{(t^4)^r}{r!}A^3 \\
           &= e^{t^4}I + te^{t^4}A + t^2e^{t^4}A^2 + t^3e^{t^4}A^3 \\
           &= e^{t^4}\left(I + tA + t^2A^2 + t^3A^3\right) \\
           &= e^{t^4} \mattwo{1-t^2}{t - t^3}{-t + t^3}{1 - t^2}
  \end{aligned}
  \]  
  Noticing that as $t \to 0$ we have $e^{tA} \to I$ is a good sanity check!
\end{solution}

% Exercise 6.5
\begin{taggedexercise}[Complete]
  Can a one-parameter group ever cross itself?
\end{taggedexercise}

\begin{solution}
  No - a one-parameter group is differentiable, hence any singularity would contradict its differentiability.
  Alternatively, by Proposition 6.17, every one-parameter group is described by $\gamma(t) = e^{tA}$ for some $A \in gl_n(\K)$, and $e^{tA}$ is injective.
\end{solution}

% Exercise 6.6
\begin{taggedexercise}[WIP]
  Describe all one-parameter groups of $\GL{1}{\C}$.
  Draw several in the x-y plane.
\end{taggedexercise}

% Need to draw some with tikz
\begin{solution}
  Let $z = a + b\mathbf{i} \in \GL{1}{\C}$.
  Then, any one-parameter group has the form
  \[\begin{aligned}
    \gamma_z(t) &= e^{tz} \\
                &= e^{t(a+ b\mathbf{i})} \\
                &= e^{at}e^{\mathbf{i}bt}
  \end{aligned}
  \]
  $e^{\mathbf{i}bt}$ can always be identified with a point on the unit circle, scaled by $e^{at}$.
  Hence, $\gamma_z(t)$ makes a spiral that "spirals" exponentially faster as $t$ increases that starts at $\gamma_z(0) = 1$.
  $z$ determines the initial condition and initial "spiral rate".

\end{solution}

% Exercise 6.7
\begin{taggedexercise}[WIP]
  Let $G = \left\{ \mattwo{x}{y}{0}{1} \in \GL{2}{\R} : x > 0 \right\}$.
  Describe the one-parameter groups in $G$, and draw several on the $xy-$plane.
\end{taggedexercise}

% Need to draw with some tikz
\begin{solution}
  First, note that given $A \in G$,
  \[
   A = \mattwo{x}{y}{0}{1}
  \]
  \[A^2 =  \mattwo{x^2}{y(x+1)}{0}{1}\]
  and
  \[A^3 = \mattwo{x^3}{y(x^2 + x + 1)}{0}{1}\]
  It can be show inductively that
  \[
  A^k = \mattwo{x^k}{y(x^{k-1} + \cdots + 1)}{0}{1}
  \]
  and so,
  \[
  \begin{aligned}
    e^{tA} &= \sum_{k=0}^\infty \frac{t^kA^k}{k!} \\
           &= \sum_{k=0}^\infty \frac{t^k}{k!} \mattwo{x^k}{y(x^{k-1} + \cdots + 1)}{0}{1} \\
           &= \sum_{k=0}^\infty \frac{t^k}{k!} \mattwo{x^k}{y\frac{x^k-1}{x-1}}{0}{1} \\
           &= \mattwo{e^{tx}}{\frac{y}{x-1}\sum_{k=0}^\infty (tx)^k/k! - t^k/k!}{0}{e^t} \\
           &= \mattwo{e^{tx}}{\frac{y}{x-1}(e^{tx} - e^t)}{0}{e^t} \\
  \end{aligned}
  \]
  No fucking way am I drawing these.
\end{solution}

% TODO
% Exercise 6.8
\begin{taggedexercise}[WIP]
  Visually describe the path $\gamma(t) = e^{t\mathbf{j}}$ in $Sp(1) \approxeq S^3$.
\end{taggedexercise}

\begin{solution}
  First, note that
  \begin{table}[ht]
    \centering
    \begin{tabular}{c|c}

    $k$ & $\mathbf{j}^k$  \\ 
    \hline
    0 & $1$ \\
    1 & $\mathbf{j}$ \\
    2 & $-1$ \\
    3 & $1$ \\
    \end{tabular}
    \end{table}

  hence, $\mathbf{j}$ has periodicity $3$.
  Next,
  \[
  \begin{aligned}
    \gamma(t) &= e^{t\mathbf{j}} \\
              &= \sum_{k=0}^\infty \frac{t^k}{k!} \mathbf{j}^k\\
              &= \sum_{i=0}^\infty \frac{t^i}{i!}1 + \sum_{j=0}^\infty \frac{t^j}{j!} + \sum_{l=0}^\infty \frac{t^l}{l!}
  \end{aligned}
  \]  
\end{solution}

% Exercise 6.9
\begin{taggedexercise}[WIP]
  Let $A = \mattwo{a}{b}{-b}{a} \in gl_n{\R}$.
  Compute $e^A$.
\end{taggedexercise}

%To fix! Exercise 6.4 is used here, and needs to be fixed + updated here
\begin{solution}
  First, note that
  \[
    \mattwo{a}{b}{-b}{a} = \mattwo{a}{0}{0}{a} + \mattwo{0}{b}{-b}{0}
  \]
  is a decomposition of $A$ into two commuting matrices.
  Thus,
  \[
  \begin{aligned}
    e^A &= \exp{\mattwo{a}{b}{-b}{a}} \\
        &= \exp\left({\mattwo{a}{0}{0}{a} + \mattwo{0}{b}{-b}{0}}\right) \\
        &= \exp\mattwo{a}{0}{0}{a} \cdot \exp\mattwo{0}{b}{-b}{0} \\
        &= \left(\sum_{k=0}^\infty \frac{a^kI^k}{k!} \right) \cdot \left(\sum_{j=0}^\infty \frac{b^k}{j!}\exp\mattwo{0}{1}{-1}{0}^j\right) \\
        &= e^{a}I \cdot e^{b^4}\mattwo{1-b^2}{b-b^3}{-b+b^3}{1-b^2} \\
        &= e^{b^4 + a}\mattwo{1-b^2}{b-b^3}{-b+b^3}{1-b^2}
  \end{aligned}
  \]
\end{solution}

% TODO
% Exercise 6.10
\begin{taggedexercise}[TODO]
  Repeat the previous problem with $A = \mattwo{a}{b}{b}{a}$
\end{taggedexercise}

% TODO
% Exercise 6.11
\begin{taggedexercise}[TODO]
  
\end{taggedexercise}

% TODO
% Exercise 6.12
\begin{taggedexercise}[TODO]
  
\end{taggedexercise}

% TODO
% Exercise 6.13
\begin{taggedexercise}[TODO]
  
\end{taggedexercise}

% TODO
% Exercise 6.14
\begin{taggedexercise}[TODO]
  
\end{taggedexercise}

%==============================================================================
\chapter{Matrix groups are manifolds}
%==============================================================================
\section{Exercises}

\begin{exercise}
Another exercise goes here.
\end{exercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\chapter{The Lie bracket}
%==============================================================================
\section{Exercises}

\begin{exercise}
Another exercise goes here.
\end{exercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\chapter{Maximal tori}
%==============================================================================
\section{Exercises}

\begin{exercise}
Another exercise goes here.
\end{exercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\chapter{Homogeneous manifolds}
%==============================================================================
\section{Exercises}

\begin{exercise}
Another exercise goes here.
\end{exercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\chapter{Roots}
%==============================================================================
\section{Exercises}

\begin{exercise}
Another exercise goes here.
\end{exercise}

\begin{solution}
Placeholder for your solution.
\end{solution}

%==============================================================================
\end{document}
